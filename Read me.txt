‚úÖ Technologies/Libraries Used:
Flask: For creating the web API.

NLTK: For natural language processing tasks like tokenization and stopword removal.

Sumy: Specifically, the LSA (Latent Semantic Analysis) summarizer.

BeautifulSoup: For web scraping (fetching article content from URLs).

CORS: To allow cross-origin resource sharing.

Requests: For making HTTP requests.

üîç What is LSA in this context?
The summarization is done using LSA (Latent Semantic Analysis) via the LsaSummarizer from the Sumy library.

LSA is an unsupervised, statistical NLP method ‚Äî not a deep learning model.

It uses Singular Value Decomposition (SVD) on a term-sentence matrix to extract the most "informative" sentences from the text.

It does not require training data or a neural network.

‚ùå What is NOT used (i.e., no DL):
No neural networks (CNNs, RNNs, Transformers, etc.)

No pretrained models like BERT, GPT, T5, etc.

No PyTorch or TensorFlow.

No word embeddings or contextual embeddings (like Word2Vec, GloVe, or BERT embeddings).

‚úÖ How to Add Deep Learning (Optional):
If you want to integrate a DL-based summarizer, you could use models like:

Model	Summary
BART	Transformer-based encoder-decoder for summarization. Available via Hugging Face.
T5	Text-to-Text Transformer, very good for summarization tasks.
PEGASUS	State-of-the-art summarization model trained specifically for abstractive summarization.

Example integration (optional):

python
Copy
Edit
from transformers import pipeline

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
summary = summarizer(content, max_length=130, min_length=30, do_sample=False)
‚úÖ Summary:
Aspect	Used?
Deep Learning	‚ùå No
Traditional NLP	‚úÖ Yes (LSA via Sumy, NLTK)
Web scraping	‚úÖ Yes (BeautifulSoup)
Web API	‚úÖ Yes (Flask)

So, to answer your question clearly:

üî∏ The DL (Deep Learning) part is not present in this project ‚Äî it uses traditional statistical NLP methods, specifically LSA, for summarization.